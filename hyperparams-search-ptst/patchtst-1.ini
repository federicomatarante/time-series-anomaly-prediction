; Classifier data

[classifier]
layers_sizes = (128,128) ; Dense layer sizes for final prediction (2 layers of 128 units)

[classifier.activation]
hidden = leakyrelu    ; Activation function for intermediate classifier layers
output = sigmoid ; Activation function for final output layer (0-1 range for anomalies)

; PatchTST encoder data

[data]         ; Information about input data
enc_in = 55    ; Number of input channels/features in the time series data

[seq]           ; Information about the time window used for analyzing input
len = 50        ; Length of the input sequence window used for making predictions
max_len = 1024  ; Maximum allowed sequence length to prevent memory issues

[pred]          ; Information about output data
len = 50        ; Number of future time steps to predict

[patch]           ; Information about the patches ( each sequence is divided in patches independently of each channel )
len = 5          ; Size of each patch segment the input sequence is divided into
stride = 5        ; Steps between consecutive patch starts (smaller = more overlap)
padding = false   ; Whether to pad sequences shorter than patch_len to match size. If False, shorter pads are excluded
padding_var = -1  ; Value used for padding invalid/missing positions

[encoder]           ; Information about the transformer encoder
layers = 4          ; Number of transformer encoder layers in the stack
heads = 8          ; Number of parallel self-attention heads per layer
dim = 128           ; Hidden dimension size for transformer representations
ff_dim = 256        ; Size of the transformer's feed-forward network
individual = false  ; If true, use separate encoders for each input feature

[attn]          ; Information about attention mechanism used
key_dim = 16    ; Dimension of key vectors in self-attention
value_dim = 16  ; Dimension of value vectors in self-attention
dropout = 0.2   ; Probability of dropping attention weights during training
residual = true ; Whether to add residual connections around attention blocks
store = false   ; Whether to save attention weights for later visualization
mask = auto     ; Attention mask type: auto/subsequent/causal/None

[dropout]       ; Dropout rates.
main = 0.1      ; Global dropout rate applied throughout model
fc = 0.1        ; Dropout rate for fully connected layers
head = 0.1      ; Dropout rate for attention heads
classifier = 0.1 ; Dropout rate in classifier layers

[norm]      ; Normalization
type = LayerNorm ; Normalization layer type: BatchNorm/LayerNorm/InstanceNorm
revin = true    ; Whether to use Reversible Instance Normalization to the input
pre = false     ; If true, normalize before operations instead of after
affine = true   ; Whether to learn scaling and bias in normalization
subtract_last = false ; Whether to subtract last value for relative processing

[decomp]        ; Whether to use decomposition for seasonal trends ( always within window )
use = false      ; Whether to decompose input into trend and seasonal components
kernel_size = 5 ; Kernel size for decomposition (must be odd number)

[pos]           ;  Positional encoding for patches
encoding = sincos ; Initial positional encoding: zeros/uniform/normal/sincos
learnable = true ; Whether position encodings can be learned during training

[head]          ; Final head for prediction
type = flatten  ; How to process final features: flatten/avg/max/conv/attention
pretrain = false ; Whether to initialize head with pretrained weights

[act]           ; Activation function
type = leakyrelu     ; Default activation function used throughout model
