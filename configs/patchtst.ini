; PatchTST forecaster data

[data]         ; Information about input data
enc_in = 76    ; Number of input channels/features in the time series data

[seq]           ; Information about the time window used for analyzing input
len = 96        ; Length of the input sequence window used for making predictions
max_len = 96    ; Maximum allowed sequence length to prevent memory issues

[pred]          ; Information about output data
len = 32        ; Number of future time steps to predict

[patch]           ; Information about the patches ( each sequence is divided in patches independently of each channel )
len = 16          ; Size of each patch segment the input sequence is divided into
stride = 16        ; Steps between consecutive patch starts (smaller = more overlap)
padding = false   ; Whether to pad sequences shorter than patch_len to match size. If False, shorter pads are excluded
padding_var = -1  ; Value used for padding invalid/missing positions

[encoder]           ; Information about the transformer encoder
layers = 3          ; Number of transformer encoder layers in the stack
heads = 4          ; Number of parallel self-attention heads per layer
dim = 64           ; Hidden dimension size for transformer representations
ff_dim = 128        ; Size of the transformer's feed-forward network
individual = True  ; If true, use separate encoders for each input feature

[attn]          ; Information about attention mechanism used
key_dim = 32    ; Dimension of key vectors in self-attention
value_dim = 32  ; Dimension of value vectors in self-attention
dropout = 0.1   ; Probability of dropping attention weights during training
residual = true ; Whether to add residual connections around attention blocks
store = false   ; Whether to save attention weights for later visualization
mask = auto     ; Attention mask type: auto/subsequent/causal/None

[dropout]       ; Dropout rates.
main = 0.1      ; Global dropout rate applied throughout model
fc = 0.1        ; Dropout rate for fully connected layers
head = 0.1      ; Dropout rate for attention heads

[norm]      ; Normalization
type = LayerNorm   ; Normalization layer type: BatchNorm/LayerNorm/InstanceNorm
revin = true    ; Whether to use Reversible Instance Normalization to the input
pre = true     ; If true, normalize before operations instead of after
affine = true   ; Whether to learn scaling and bias in normalization
subtract_last = false ; Whether to subtract last value for relative processing

[decomp]        ; Whether to use decomposition for seasonal trends ( always within window )
use = true      ; Whether to decompose input into trend and seasonal components
kernel_size = 13 ; Kernel size for decomposition (must be odd number)

[pos]           ;  Positional encoding for patches
encoding = sincos ; Initial positional encoding: zeros/uniform/normal/sincos
learnable = true ; Whether position encodings can be learned during training

[head]          ; Final head for prediction
type = flatten  ; How to process final features: flatten/avg/max/conv/attention
pretrain = false ; Whether to initialize head with pretrained weights
output_act = sigmoid ; Output activation function (options: sigmoid, tanh, linear)

[act]           ; Activation function
type = leakyrelu     ; Default activation function used throughout model
