# PatchTST Model Training Configuration

[training]
seed = 534112
max_epochs = 100                 ; Maximum number of training epochs
batch_size = 128                ; Number of samples in each training batch
precision = 32                  ; Numerical precision for training (options: 32, 16, mixed)
accumulate_grad_batches = 1     ; Number of batches to accumulate gradients over
val_check_interval = 0.1        ; Validation check frequency (1.0 = once per epoch, can be ; 1.0)
log_every_n_steps = 20          ; Number of training steps between logging updates
num_workers = 2              ; Number of workers to use for dataset loading
gradient_clip_value = 0         ; Numerical value of the gradient clip
loss_scaling_factor = true      ; Whether to use or not the scaling factor in the loss
loss_weight = 10               ; Weight to give to samples with at least one anomaly

[early_stopping]
monitor = val_loss              ; Metric to monitor (options: val_loss, val_existence, val_density, val_leadtime, val_dice)
patience = 5                    ; Number of epochs to wait for improvement before stopping
mode = min                      ; Whether to minimize or maximize the metric (options: min, max)
min_delta = 0.01                ; Minimum change to qualify as an improvement
verbose = true                  ; Whether to log early stopping information or not

[model_checkpoint]
save_directory = checkpoints/   ; Directory to save model checkpoints
filename = patchtst-{epoch:02d}-{val_loss:.4f}  ; Format for checkpoint filenames
monitor = val_loss              ; Metric to monitor for saving checkpoints
mode = min                      ; Whether to minimize or maximize the metric (options: min, max)
save_top_k = 3                  ; Number of best models to keep
save_last = true                ; Whether to additionally save the last model

[model]
threshold = 0.1                 ; Threshold for binary classification decisions (0.0 to 1.0)

[metrics]
threshold = 0.1                   ; Threshold for binary classification decisions (0.0 to 1.0)
train_metrics_threshold = 0.1     ; Threshold to use for training metrics (0.0 to 1.0)
val_metrics_threshold = 0.1       ; Threshold to use for validation metrics (0.0 to 1.0)
evaluate_metrics_threshold = 0.1  ; Threshold to use for evaluation metrics (0.0 to 1.0)

[hardware]
num_devices = 1               ; Number of GPUs/devices to use
accelerator = gpu             ; Hardware accelerator to use (options: auto, gpu, cpu, tpu)

[logging]
enabled = true                ; Whether to enable logging (options: true, false)
save_directory = logs/        ; Directory to save logs

[scheduler]
type = StepLR                 ; Learning rate scheduler type (options: ReduceLROnPlateau, StepLR, CosineAnnealingLR, OneCycleLR, ExponentialLR, CosineAnnealingWarmRestarts)
frequency = 10                 ; How often to update the learning rate (in intervals)
interval = epoch              ; Epoch or Step # TODO document better

[scheduler.StepLR]
step_size = 5               ; Period of learning rate decay (epochs)
gamma = 0.1                 ; Multiplicative factor of learning rate decay
verbose = true
last_epoch = -1

[optimizer]
type = Adam                   ; Optimizer algorithm (options: Adam, SGD, RMSprop, AdamW, Adadelta, Adagrad, RAdam)

[optimizer.Adam]
lr = 1e-5                     ; Initial learning rate
betas = (0.9, 0.999)          ; Coefficients for computing running averages of gradient and its square
eps = 1e-8                    ; Term added to denominator for numerical stability
weight_decay = 0.01           ; L2 regularization factor (0.0 means no regularization)
amsgrad = false               ; Whether to use the AMSGrad variant of Adam
