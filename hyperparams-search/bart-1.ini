[data]
input_length = 50 ; Length of input sequence
channels = 55 ; Number of input channels (features) in your data
output_length = 50 ; Length of output sequence for predictions
patch_size = 5 ; Number of timesteps/datapoints to group together in each patch ( Tested )

[embedding]
d_embed = 256 ; Dimension of the embedding space in the transformer
positional_encoding = abs  ; Type of positional encoding to use (sin, abs)
relative_position_embedding = True ; Whether to use relative position embeddings


[transformer_block]
transformer_n_layer = 12 ; Number of transformer layers in the encoder ( Tested )
transformer_n_head = 8   ; Number of attention heads in each transformer layer ( Tested )
hidden_dim_rate = 2.0    ; Multiplier for hidden dimension ( hidden_dimensions = hidden_dim_rate * d_embed )
causal_mask = False      ; Causal mask ( Tested )

[training]
dropout = 0.1     ; Dropout rate for regularization
